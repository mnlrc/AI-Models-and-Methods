\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{eso-pic,graphicx}
\usepackage[a4paper,left=2cm,right=2cm,top=2cm,bottom=2cm, footnotesep=3cm]{geometry}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{float}
\usepackage{subcaption}
\usepackage[pages=some]{background}

\newcommand{\HRule}{\rule{\linewidth}{0.3mm}}


% -----------------------------------------------------

\begin{document}

\AddToShipoutPictureBG*{\includegraphics[width=\paperwidth,height=\paperheight]{../../general-images/background.png}}
\clearpage
\begin{titlepage}
  \begin{sffamily}
  \begin{flushleft} \large
    \includegraphics[height=2.0cm]{../../general-images/logo_ulb.jpg}
    \vspace{5cm}
   \end{flushleft}
  \begin{center}

    %Title
        \textsc{\huge INFO-F311 - Projet d'IA 3}\\[1cm]

    \HRule \\[0.7cm]

        \textsc {\Huge Apprentissage par renforcement}\\[0.4cm]

    \HRule \\[1.2cm]

% Author and supervisor
\begin{figure}[h]
\addtocounter{figure}{-1}
\begin{subfigure}{0.5\textwidth}
\begin{flushleft} \large
\emph{Auteur:}\\
Manuel \textsc{Rocca} - 000596086\\


\end{flushleft}
\end{subfigure}
\end{figure}
\vspace{1cm}

\begin{figure}[h]
\addtocounter{figure}{-1}
\begin{subfigure}{0.4\textwidth}
\begin{flushright} \large
\emph{Professeurs:} \\
Tom  \textsc{Lenaerts}\\
\emph{Assistants:} \\
Axel \textsc{Abels} \\
Martin \textsc{Colot} \\
Yannick \textsc{Molinghen} \\
Pascal \textsc{Tribel}
\end{flushright}
\end{subfigure}
\end{figure}


    \vfill

    %Bottom of the page
    {\large Année académique 2025-2026}
  \end{center}

  \end{sffamily}
\end{titlepage}


\clearpage


\tableofcontents
\newpage

% -----------------------------------------------------

\section{Introduction}

Pour ce troisième projet du cours d'Intelligence Artificielle, nous avons implémenté des algorithmes d'apprentissage par renforcement (\emph{reinforcement learning} en anglais). Nous utilisons toujours l'environnement \emph{LLE} comme pour les parties 1 et 2. 

Trois différences (ou ajouts) sont introduites: les cases tourbillons (où l'agent meurt) la fonction de récompense, et surtout, l'aspect stochastique, non-déterministe de l'environnement. 

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{graphs/initial_env.png}
    \caption{Environnement utilisé}
    \label{fig:env}
\end{figure}

\subsection{Environnement stochastique}

Un environnement stochastique est un environnement où, lorsque l'agent effectue une action,
nous ne sommes pas certains de l'état dans lequel il va arriver. En pratique, si un agent dans un état $s$ souhaite effectuer une action $a$ pour arriver dans un nouvel état $s'$, il y a une probabilité $p$ qu'une autre action aléatoire soit prise.

\subsection{La fonction de récompense}

Une fonction de récompense est une fonction qui, pour chaque transition d'état, octroie une valeur. En d'autres termes, une valeur numérique est associée à chaque action prise par l'agent afin de le diriger de manière idéalement souhaitée. 

Associer une valeur adéquate est tout sauf quelque chose de simple. Des comportements inattendu peuvent survenir si cette fonction est mal définie (par exemple: si un agent doit atteindre une sortie avec une récompense positive mais que le coût de vie, le coût par pas est très élevé, il aura plus tendance à chercher un moyen de faire le moins de pas possible en trouvant un endroit plus proche pour mourir permettant une minimisation de son score total).


\section{Value Iteration}

L'algorithme \emph{Value Iteration} utilise l'équation de Bellman (\ref{eq:bellman}) de manière itérative pour mettre à jour les valeurs estimées de chaque état de l'environnement.

\begin{figure}[h]
\centering
\begin{equation}
    \label{eq:bellman}
    V^{*}(s) = \max_{a} \sum_{s'} T(s,a,s') \,\big[ R(s,a,s') + \gamma V^{*}(s') \big]
\end{equation}
\caption*{Équation de Bellman caractérisant les valeurs optimales}
\end{figure}
Les itérations de l'algorithme sont bornées par une valeur $\delta$ comme suit:

\begin{equation}
    \max_{s \in S} |V_{k+1}(s) - V_k(s)| < \delta
\end{equation}

En d'autres termes, à chaque itération, la différence/variation entre les valeurs des états précédents et les nouvelles valeurs est calculée en appliquant
l'équation \ref{eq:bellman}. Si celle-ci est inférieure du seuil $\delta$ donné, l'algorithme s'arrête. \\
L'algorithme converge après un certain nombre d'itérations. Ce nombre peut être, dans certains cas, très grand voire quasi infini. C'est pourquoi il est intéressant d'établir un seuil de variation maximal afin d'arrêter l'algorithme après une durée d'exécution raisonnable. Comme nous l'avons vu au cours théorique, parfois un dixième des itérations suffisent à obtenir des valeurs pratiquement égales aux valeurs obtenues à la convergence.

\subsection{Entraînement de l'algorithme}

Dans cette section, nous nous intéressons aux valeurs d'états finales obtenues par notre implé-mentation de l'algorithme \emph{Value Iteration} pour des valeurs de $\delta \in \{1, 0.1, 0.01, 0.005, 0.001\}$ ainsi que le nombre d'itérations $k$. Pour chaque valeur de $\delta$ testée, nous usons comme facteur de réduction $\gamma$ = 0.9.
\begin{table}[h]
    \centering
    \label{tab:delta_k}
    \begin{tabular}{ | r | l | }
        \hline
        $\delta$ & $k$ \\ \hline
        1 & 10 \\ \hline
        0.1 & 14 \\ \hline
        0.01 & 18 \\ \hline
        0.005 & 18 \\ \hline
        0.001 & 20 \\ \hline
    \end{tabular}
    \caption{Nombre d'itérations de l'algorithme \emph{Value Iteration} en fonction d'un $\delta$ donné.}
\end{table}

Plus il y a d'itérations, plus nous nous approchons de la convergence. Voici les heatmaps pour $\delta$ = 1 et $\delta$ = 0.001 affichant les valeurs d'états dans l'environnement stochastique après $k$ itérations: 

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=\linewidth]{graphs/delta=1.png}
        \caption{$\delta$ = 1, $k$ = 10}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=\linewidth]{graphs/delta=0_001.png}
        \caption{$\delta$ = 0.001, $k$ = 20}
    \end{subfigure}
    \caption{Heatmaps générées par l'algorithme \emph{Value Iteration}}
\end{figure}

Nous nous rendons très vite compte que la valeurs des états les plus éloignés de l'état initial en $(0, 6)$ (en haut à droite) convergent plus rapidement que ceux plus proches de l'état d'origine.
Cela s'explique par le fait que toutes les valeurs d'états calculées par \emph{Value Iteration} auront une valeur nulle tant qu'une
récompense n'a pas été trouvée. Démontrons cela de manière concise:

\begin{proof}[Démonstration]
    En observant l'équation \ref{eq:bellman}, il est clair que la valeur est nulle dans deux cas:
    \begin{itemize}
        \item si $T(s,a,s')$ est nulle. Or cette valeur n'est jamais nulle, il y a toujours une probabilité donnée de passer d'un état $s$
        à un état $s'$ suivant.
        \item si $R(s,a,s') + \gamma V^{*}(s')$ est nul. Initialement tous les $V(s)$ sont nuls, donc le seul terme qui peut donner une
        valeur non-nulle à cette expression est la récompense $R(s,a,s')$ ($\gamma$ est une constante non-nulle).
    \end{itemize} 

    En somme, les valeurs d'état calculées se propagent à partir des transitions accordant une récompense expliquant ainsi la convergence
    plus rapide dans ces environs.
\end{proof}

Pour conclure cette section nous souhaitons proposer une analyse du nombre d'itérations requises pour atteindre la convergence.
Pour ce faire, nous avons changé la condition d'arrêt de notre algorithme. Dorénavant, il s'arrête dès que la variation entre les
valeurs d'états de deux itérations successives est nulle. Ce faisant, nous sommes arrivés à $k$ = 50 (et une heatmap exactement identique
à celle obtenue avec un $\delta$ = 0.001 en $k$ = 20 itérations).

Ceci complète donc l'hypothèse fournie dans au cours théorique concernant l'utilité d'un grand nombre d'itérations. En effet, après la moitié 
du nombre d'itérations requises pour obtenir la convergence nous obtenons déjà des valeurs similaires voire identiques.


\subsection{Stratégie optimale}

Récupérer la stratégie optimale consiste à récupérer la meilleure action pour chaque état. Cela est
aisément faisable en créant une nouvelle matrice $optimal\_policy = np.zeros(shape=env.map.size)$.
Ensuite, au lieu d'y placer, pour un état $s$ donné, la meilleure valeur pour cet état, nous y plaçons
la meilleure action associée à cette meilleure valeur.

La politique optimale peut changer en fonction du paramètre $p$ de l'environnement (\ref{fig:env}).
Voici nos résultats pour $p = 0.1$ et $p = 0.9$:

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=\linewidth]{graphs/policy-p=0.1.png}
        \caption{$p = 0.1$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=\linewidth]{graphs/policy-p=0.9.png}
        \caption{$p = 0.9$}
    \end{subfigure}
    \caption{Politiques optimales trouvées par l'algorithme \emph{Value Iteration} avec $\delta = 0.01$}
\end{figure}

Nous observons que l'agent préfère se donner la mort plutôt que d'atteindre une sortie même s'il se trouve à côté
de celle-ci. Expliquons cela analytiquement à l'aide d'un exemple concret:

\begin{proof}[Exemple]
    Considérons un agent en (0, 5). Trois actions s'offrent à lui: rester, ouest, sud. Aller au sud
    lui donnerait une récompense de 10. Sauf que cette récompense de 10 est fortement dévaluée par $p$
    lorsque nous considérons sud comme action. Lorsque nous considérons ouest par exemple, la récompense
    sera une fois de plus dévaluée de $90\%$ mais la récompense étant 0, cela import peu. En revanche,
    comme la formule indique qu'il faut faire la somme de tous les états $s'$ nous repassons sur l'action
    sud et la récompense de 10 est cette fois ci dévaluée de $\frac{p}{len(available\_actions)}$. Cet élément
    pèse donc fort dans la somme lorsque l'on considère l'action ouest expliquant ainsi pourquoi l'agent prend
    cette action au lieu de l'action sud.
\end{proof}

Nous en concluons que les récompenses sont tellement dévaluées que l'agent préfère prendre une action allant
à l'opposé de la sortie et se donner la mort.

\section{Q-learning}

L'algorithme \emph{Q-learning} calcule, pour chaque état et pour chaque action possible à partir de cet état, une valeur réelle. À partir
de cette valeur réelle, il détermine une stratégie optimale à suivre en prenant l'action ayant la valeur la plus élevée pour
chaque état. 

\begin{figure}[h]
\centering   
    \begin{equation}
    Q(s,a) \leftarrow (1 - \alpha)\,Q(s,a)
    + \alpha \big[ R(s,a,s') + \gamma\,V(s') \big]
    \label{eq:q-learning}
    \end{equation}
\caption*{Mise à jour de la \emph{q-value} en un état $s$ en prenant une action $a$, avec $ V(s) = \max_{a \in A} Q(s,a)$}
\end{figure}



Contrairement à \emph{Value Iteration} qui ne considère que des transitions en n'interagissant pas avec l'environnement, \emph{Q-learning}
fait un apprentissage \emph{offline} en interagissant directement avec l'environnement. En d'autres termes, si l'agent
souhaite déterminer la récompense obtenue en passant de son état $s$ à un nouvel état $s'$ avec une action $a$ il doit 
exécuter cette action. Donc s'il doit mourir pour savoir que ce n'est pas bon, il le fera.

Une autre différence par rapport au \emph{Value Iteration} est le choix des actions. Le \emph{Q-learning} choisit l'action suivante
sur base d'une politique bien déterminée. Il nous a été demandé d'en implémenter deux:

\begin{itemize}
    \item Exploration $\varepsilon$-greedy: il y a une probabilité $\varepsilon$ de choisir une action aléatoire et une probabilité
    $1 - \varepsilon$ de prendre une action qui maximise $Q(s, a)$. Ceci est formalisé par l'équation fournie dans les consignes reprise
    ci-dessous (avec $r$ un nombre uniformément aléatoire entre 0 et 1):
        \begin{equation}
            \centering
            \label{eq:epsilongreedy}
            \pi(s) =
            \begin{cases}
                \arg\max_{a \in A} Q(s,a) & \text{si } r \le \varepsilon \\
                a \sim A & \text{sinon}
            \end{cases}
        \end{equation}
    En fait, $\varepsilon$ est une variable qui, plus elle est élevée, plus elle favorise l'exploration et, plus elle est basse,
    plus elle favorise l'exploitation. L'exploration est lorque l'agent teste une action aléatoire indépendamment des \emph{q-values}
    déjà calculées pour en découvrir de nouvelles (et potentiellement découvrir d'autres récompenses). L'exploitation fait tout l'inverse.
    Elle choisit l'action avec la \emph{q-value} la plus élevée. Elle "ne prend pas de risques". 
    \item Exploration softmax: utilise la fonction softmax adaptée à notre environnement. L'é-quation fournie dans les consignes est
    la suivante (avec $\tau$ la "température"):
        \begin{equation}
            \pi_{a}(s) = 
            \frac{ e^{\frac{Q(s,a)}{\tau}} }
                 { \sum_{a' \in A} e^{\frac{Q(s,a')}{\tau}} }
        \label{eq:max-Boltzmann}
        \end{equation}
    En termes concrets, pour un état $s$ donné, l'équation va calculer, pour chaque action $a$ disponible, une certaine probabilité.
    Donc chaque action sera pondérée d'une probabilité $p$. Nous choisissons ensuite une action aléatoirement en prenant en compte des
    poids associés. 
\end{itemize}

\subsection{Entraînement de l'algorithme}

Nous analysons dans cette section les performances de l'agent dans l'environnement en utilisant l'algorithme \emph{Q-learning}
sur 20 000 itérations, interactions avec l'environnement en utilisant des paramètres et des politiques différentes.

Les paramètres généraux sont:
\begin{itemize}
    \item l'environnement: probabilité de prendre une action aléatoire au lieu de celle choisie $p$ = 0.1
    \item l'algorithme: \emph{learning rate}\footnote{Taux définissant à quel point une ancienne valeur remplace la nouvelle; un taux plus élevé permet à
    l'agent de s'adapter plus rapidement aux nouvelles valeurs calculées tandis qu'un taux plus bas nous aurons des résultats plus stables
    mais un apprentissage plus lent.} $\alpha$ = 0.1 et \emph{discount factor} $\gamma$ = 0.9
    \item $random.seed(0)$
\end{itemize}

\subsubsection{Exploration $\varepsilon$-greedy}

Comme exprimé ci-dessus, chaque expérience comporte 20 000 itérations. Nous utilisons un $\varepsilon$ différent par expérience.
Nous en faisons quatre. Pour les trois premières, nous utilisons $\varepsilon \in \{0, 0.1, 0.5\}$ tandis que pour la
dernière nous faisons diminuer $\varepsilon$ linéairement de 1 à 0.01 au cours de l'entraînement.

\subsubsection{Exploration softmax}

Pour cette politique d'exploration nous utilisons les valeurs de température $\tau \in \{0.01, 1, \\10\}$ pour
les trois premières expériences. La dernière fait usage d'une fonction diminuant exponentiellement\footnote{Pour ce faire nous
avons utilisé l'expression $f(x) = f(x_0)e^{-kx}$. Nous avons $f(x)$, $x$ et $x_0$. Il suffit donc de résoudre une
équation pour obtenir finalement $k$ = $\frac{4\ln(10)}{20000}$.} allant de 100 à 0.01 au cours de l'entraînement.


\subsubsection{Résultats}

Voici les graphiques reprenant les résultats des huit expériences menées:

\begin{figure}[H]
    \centering
    \centering
    \includegraphics[width=\linewidth]{graphs/Average score per episode (logarithmic scale).jpg}
    \caption{Score moyen par épisode de l'algorithme \emph{Q-learning} avec différentes valeurs par politique d'exploration.}
    \hfill
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{graphs/Total score after 20_000 iterations (logarithmic scale).jpg}
    \caption{Score total pour une exécution de l'algorithme \emph{Q-learning} avec différentes valeurs par politique d'exploration.}
\end{figure}

Nous considérons ici un épisode comme toutes les actions prises par l'agent tant qu'il ne meurt pas. En effet,
lorsque l'agent meurt, l'environnement est réinitialisé.

Nous observons que lorsque le score moyen est élevé, le score total est très bas voire négatif.
Ceci s'explique par l'opposition entre exploitation et exploration. Si l'algorithme ne fait que
exploiter (en particulier la sortie la plus proche avec une faible récompense), il aura un score
moyen acceptable mais un score total assez mauvais. À l'opposé, une forte exploration implique de nombreuses
morts de l'agent expliquant le score moyen faible et un score total élevé\footnote{Ces résultats se reflètent
également dans le nombre de pas moyen effectués par l'agent avant de mourir. Une forte exploitation implique
peu de risques et donc beaucoup d'étapes avant de mourir (donnée affichée à l'exécution.)}. 


\section{Discussion}

Cette section vise à répondre aux questionnements initiés à la section 4.3.\ des consignes.

\subsection{Aspect aléatoire}

L'algorithme \emph{Value Iteration}, ne comprend pas d'aspect aléatoire. En effet pour calculer la valeur 
d'un état $s$, nous considérons chaque action $a$ possible à partir de $s$ pour arriver à différents états 
$s'$. Chaque action possède une certaine probabilité d'être effectuée mais comme nous travaillons sur toutes
les transitions possibles (la somme de tous les états $s'$), nous arrivons à "une probabilité totale de 1".
Il faut plutôt voir les probabilités associées à chaque action $a$ comme un poids dans l'expression permettant
de mieux valuer une transition d'un état $s$ à un état $s'$ avec une action $a$. Pour concrétiser tout ça,
nous avons retiré la seed rendant ainsi notre algorithme en théorie aléatoire. Or, à chaque exécution (avec
les mêmes paramètres bien entendu), nous avons obtenus des heatmaps complètement identiques. \\

\noindent À l'opposé, l'algorithme \emph{Q-learning} interagit directement avec l'environnement qui possède une
probabilité $p$ d'effectuer une action aléatoire au lieu de l'action souhaitée. De plus, les politiques d'exploration
ont également un aspect aléatoire. En effet, la politique $\varepsilon$-greedy comprend une probabilité $\varepsilon$
de choisir une action aléatoire (encourageant l'exploration) et, la politique softmax qui calcule un poids, une
probabilité pour chaque action avant d'en sélectionner une aléatoirement en prenant en compte le poids calculé.

\subsection{Stratégies d'exploration}

\subsubsection{Analytiquement}
\label{sec:analytic}

La stratégie $\varepsilon$-greedy balance son exploration et sa maximisation à l'aide du paramètre $\varepsilon$. En effet,
une action aléatoire est choisie avec une probabilité $\varepsilon$, encourageant l'exploration de l'environnement. À l'opposé,
il y a une probabilité $1 - \varepsilon$ qu'une action maximisant les \emph{q-values} soit prise, encourageant ainsi l'exploitation.

Concernant \emph{Max-Boltzmann}, le paramètre $\tau$ est celui influençant la prise de décision. Lors-que $\tau \rightarrow \infty$,
nous observons aisément que, dans l'équation \ref{eq:max-Boltzmann}, nous avons un dividende ayant une valeur tendant vers
$e^{0} = 1$ et le dénominateur a une valeur tendant vers $\sum_{a} e^{0} = \sum_{a} 1$. Donc, pour chaque action $a \in A$, nous avons
une même probabilité $\frac{1}{len(A)}$ encourageant l'exploration. À l'inverse, quand $\tau \rightarrow 0$, les probabilités
associées à chaque action $a$ dépendent plus des \emph{q-values} (au numérateurs des exposants), poussant l'algorithme à
l'exploitation.

\subsubsection{Expérimentalement}

L'analyse analytique proposée dans la section \ref{sec:analytic} précédente est appuyée par les résultats\footnote{Afin de ne
pas surcharger ce document d'avantage, nous analysons les résultats sans ajouter les heatmaps. Cependant, nous proposons les
paramètres d'entrée afin de permettre au lecteur de reproduire les expériences. De plus, toutes les heatmaps sont
disponibles dans le directory \emph{./graphs}.} obtenus avec les paramètres suivants:

\begin{itemize}
    \item random.seed = 0
    \item $\gamma$ = 0.9
    \item $\alpha$ = 0.1
    \item nombre d'étapes = 20\_000
\end{itemize}

$\varepsilon$-greedy explore bien comme prévu plus $\varepsilon$ est faible. Plus $\varepsilon$ augmente, moins
il explore. Ceci est visible sur la heatmap avec $\varepsilon = 0.9$: les cases à côté de la première sortie (la plus proche)
sont jaunes (valeur élevée) tandis que la sortie la plus lointaine voit ses cases voisines vierges de valeurs; l'exploration
n'a pas été suffisament profonde. Finalement, concernant l'expérience avec $\varepsilon$ diminuant linéairement, nous devrions
avoir une heatmap relativement bien explorée ($\varepsilon$ élevé au début de l'algorithme) avec une exploitation plus
poussée vers la fin. Cependant, avec 20\_000 itérations, l'algorithme ne trouve pas la sortie la plus éloignée. Nous avons
alors essayé avec 50\_000 itérations et nous voyons immédiatement que la sortie lointaine est découverte et a une case très
jaune à proximité. La conclusion est donc la suivante: sur plus d'itérations, l'algorithme a eu plus de temps pour explorer,
plus d'itérations avec un $\varepsilon$ relativement élevé pour finalement bien exploiter les cases aux \emph{q-values} les plus
élevées.

La stratégie \emph{Max-Boltzmann}, comme $\varepsilon$-greedy colle à la théorie: plus $\tau \rightarrow 0$, moins il y a
d'exploration. Cependant, nous observons que avec une valeur de $\tau$ favorisant l'exploration ou l'exploitation,
des \emph{q-values} sont calculées de manière à peu près équitable partout dans l'environnement (sauf pour les valeurs extrêmes
comme par exemple $\tau = 0.01$ où les \emph{q-values} ne s'étendent pas jusqu'à la sortie éloignée).


\subsubsection{Conclusion}

Ces analyses permettent donc de répondre à la question concernant l'équilibre des politiques d'exploration. La stratégie d'exploration
\emph{Max-Boltzmann} semble atteindre un meilleur équilibre entre exploration et exploitation. Chaque action possède une probabilité
propre d'être sélec-tionnée. Donc, même si une action possède une faible probabilité, elle peut se voir être choisie tout de même. En
d'autres termes, les actions ayant une \emph{q-value} plus élevées sont favorisées mais pas choisies exclusivement. De plus,
ces calculs de probabilité sont pondérés par le paramètre $\tau$ permettant de diriger, de choisir une approche tendant plus vers
l'exploitation ou l'exploration (mais jamais un des deux exclusivement).

En effet, à son opposé, la stratégie
$\varepsilon$-greedy est binaire. Il y a une probabilité $\varepsilon$ de prendre une action aléatoire. Donc, avec $\varepsilon$
proche de 1, nous avons un algorithme qui explore majoritairement en négligeant l'exploitation (et inversément pour $\varepsilon$
proche de 0). On ne retrouve donc pas la même stabilité que chez \emph{Max-Boltzmann}.

\subsection{Effet du discount factor $\gamma$}

Nous pouvons exprimer le \emph{discount factor} comme un facteur qui diminue l'importance des valeurs des états futurs $s'$
atteints à partir de $s$ en effectuant l'action $a$.

Dans le cas de notre programme, comme nous l'avons déjà expliqué, les valeurs d'état se "propagent" à partir des transitions d'un
état $s$ à un état $s'$ avec une action $a$ qui octroient une récompense. Diminuer $\gamma$ aura pour effet
de réduire ce phénomène de propagation et diminue donc les valeurs des états en général. De ce fait, les cases
des heatmaps (pour \emph{Value Iteration} et \emph{Q-learning}) auront de plus petites valeurs et donc des cases avec des couleurs
plus faibles. Pour des valeurs de $\gamma$ particulièrement faibles (ex: $\gamma = 0.1$) nous observons que seules les
sorties sont claires, en particulier celles avec une récompense plus élevée si celle-ci est atteinte (exploration suffisante requise).

\subsection{Effet du taux d'apprentissage $\alpha$}

Le taux d'apprentissage balance les valeurs \emph{q-values} précédemment calculées et les nouvelles (valeur d'état $s'$ et la récompense obtenue
en arrivant à cet état $s'$). Nous observons à l'équation \ref{eq:q-learning} que, plus $\alpha$ est élevé, plus 
les nouvelles valeurs auront un poids important dans le calcul du nouveau poids et moins 
l'ancienne valeur d'état aura de poids (et inversément pour $\alpha$ faible).

En particulier, nous observons que, en comparant un $\alpha = 0.9$ et $\alpha = 0.1$ sur la stratégie $\varepsilon$-greedy 
avec $\varepsilon = 0.1$ (exploration très faible), les \emph{q-values} sont mieux réparties. En effet, avec $\alpha = 0.1$,
uniquement les cases proches des sorties sont colorées et possèdent donc une valeur non-nulle ou élevée.

Nous en tirons que, plus $\alpha$ est élevé, plus les \emph{q-values} précédemment calculées sont écrasées. Ceci
est justifié par notre exemple ci-dessus. Avec $\varepsilon = 0$, $\varepsilon$-greedy n'explore que peu et exploite
surtout. Avec $\alpha = 0.1$ le chemin menant aux sorties possède des \emph{q-values} quasi nulles car, l'algorithme
prend toujours la même action et ne calcule que rarement les \emph{q-values} éloignées des sorties. Avec une
propagation plus importante, le chemin est donc plus éclairé car, même pour un passage unique par un chemin, même pour une seule 
exploration, les valeurs précédentes ne sont pratiquement pas prises en compte; uniquement la récompense compte. 

\section{Conclusion}

Ce troisième projet d'Intelligence Artificielle nous a permis une fois de plus d'appliquer un concept vu en cours,
l'analyser et conclure. Nous avons pu peser les avantages et inconvénients de chaque algorithme d'apprentissage
par renforcement implémenté, à savoir le \emph{Value Iteration} et le \emph{Q-learning}. L'analyse
des heatmaps générées nous a permis de visualiser les algorithmes de façon claire permettant ainsi une compréhension
accrue de ce qui nous a été demandé.


\end{document}
